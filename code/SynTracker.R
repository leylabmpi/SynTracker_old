#important points:
# 1. the title of each region to be compared is the Refseq title of the choromosome + the location of the region along the sequence, 
# as generated by this python script: select_1kb_core_sequences_for_synteny_ref_genomes.ipynb
# 2. The title of the contigs in each assembly should be named with the Sample ID in the begginging: e.g, ID_NODE_x_coverage_y, etc. 


#requirements
library(DECIPHER)
library(tidyverse)
library(phangorn)
library(ggtree)
library(parallel)
library(ggpubr)

#inputs required:
# 1. metadata: metadata file, should include the sample ID, and any other relevant fields, titled "GroupA", "GroupB", etc. 
# 2. paths: folder that includes subfolders, in each subfolder are fasta files (each file contains homologos sequences, matching to one central region) -> output of the Blastcmddb search in the bash script


metadata<-read.csv(file="/ebio/abt3_projects/Strain_tracking_synteny_blocks/analysis/cophylogeny/cophylogeny_metadata_mod.csv", sep=";", header = TRUE) 
paths<-list.files(path="/ebio/abt3_projects/Strain_tracking_synteny_blocks/analysis/github_test/blastcmddb", full.names=T)
pathnames<-"" #for example: "/ebio/abt3_projects/Strain_tracking_synteny_blocks/analysis/github_test/blastcmddb_output"
for (j in 1:length(paths)) {pathnames[j]<-str_split(paths[j], "[/.]")[[1]][8]} #change the last element returened by str_split to match your path: the goal is to return the folders that hold the sequences from samples that match each central region
names(paths)<-pathnames


##########################
#. external_func: a wratpper function to run the SynTracker function
# inputs:
# 1. paths: list of folder paths
# 2. path_names: the names of the elements in "paths"
# 3. metadata: the metadata file

external_func<-function(paths, path_names, metadata) {
  #create a temp folder 
  tmp_folder<-runif(1, 1000000000000, 9999999999999) %>% round %>% as.character %>% paste0("tmp", ., "/") 
  dir.create(tmp_folder)

  # first part: run the synteny_analysis function on each input file
  filepaths <-list.files(path=paths, full.names=TRUE)
  gene_names<-""
  for (i in 1:length(filepaths)) {gene_names[i]<-str_split(filepaths[i], "[/]")[[1]][9]} # Here again, the element returend by str_split should be adjusted to match your specific path
  objects<-mcmapply(synteny_analysis, filepaths, gene_names, tmp_folder, SIMPLIFY = F, mc.preschedule=T, mc.cores=6) #change number of cores if needed
  names(objects)<-gene_names 
  bad_objects_elements <- sapply(objects, inherits, what = "try-error") #identify iterations of synteny_anlysis that failed for some reason.
  objects<-objects[!bad_objects_elements] # and filter these elements out...
  narrow<-Filter(function(x) nrow(x) > 1, objects) #filter elemnts with comparisons of less than 2 valid seqs, if this happened.
  #print(length(objects))
  rm(objects)
  
  # second part: Process synteny objects
  dfs<-mcmapply(synteny_scores,narrow, MoreArgs = list(metadata), SIMPLIFY = F, mc.preschedule=F,mc.cores=6) #change number of cores if needed
  bad_dfs_elements <- sapply(dfs, inherits, what = "try-error") #identify iterations of synteny scores that failed for some reason. Mostly (although very rare), those are two hits for the same region
  dfs<-dfs[!bad_dfs_elements] # and filter these elements out...
  

  #third part: add names to each table in a new column, merge to one big dataframe, arrange it.
  improved_dfs<-map2(dfs, names(dfs), add_names)
  big_dfs<-bind_rows(improved_dfs)  # bind to one dataframe
  
  # change order of sample1 and sample2, according to some rules, so that the order will be uniform throughout the table
  # i.e. sampleX-sampleY will always be like that and not sampleY-sampleX ==> if the order is not uniform it will be treated as two different comparisons
  big_dfs<-big_dfs %>% mutate(replaced = ifelse(sample2>sample1, "yes", "no")) # add a column specifing if the order of sample 1 and 2 should be replaced (for the sake of Grouping correctly in the next lines)
 
  #change the order of sample specific fields: YOU SHOULD modify them if your metadata file contains different numer of fields
   big_organized_dfs<-big_dfs %>% mutate(temp=ifelse(replaced == "yes", as.character(sample1), "no need"),  #if replaced == yes: temp will hold sample1
                                        sample1 = ifelse(replaced == "yes", as.character(sample2), as.character(sample1)), #sample1 will hold sample2
                                        sample2=ifelse(replaced == "yes", temp, as.character(sample2)),  #sample2 will hold temp (the original sample1...)
                                        temp=ifelse(replaced == "yes", as.character(GroupA1), "no need"),
                                        GroupA1 = ifelse(replaced == "yes", as.character(GroupA2), as.character(GroupA1)),
                                        GroupA2=ifelse(replaced == "yes", temp, as.character(GroupA2)),
                                        temp=ifelse(replaced == "yes", as.character(GroupB1), "no need"),
                                        GroupB1 = ifelse(replaced == "yes", as.character(GroupB2), as.character(GroupB1)),
                                        GroupB2=ifelse(replaced == "yes", temp, as.character(GroupB2)),
                                        temp=ifelse(replaced == "yes", as.character(GroupC1), "no need"),
                                        GroupC1 = ifelse(replaced == "yes", as.character(GroupC2), as.character(GroupC1)),
                                        GroupC2=ifelse(replaced == "yes", temp, as.character(GroupC2)),
                                        temp=ifelse(replaced == "yes", as.character(GroupD1), "no need"),
                                        GroupD1 = ifelse(replaced == "yes", as.character(GroupD2), as.character(GroupD1)),
                                        GroupD2=ifelse(replaced == "yes", temp, as.character(GroupD2))
  ) 
  
  #return(big_organized_dfs)

  
  ##################
  # filter and subsample regions
  ##################
  
  # find the maximal number of regions/pairwise-comparison:
  biggest_group<-max(big_organized_dfs %>% 
                     group_by(sample1, sample2, GroupA1, GroupA2, GroupB1, GroupB2, is.same.GroupA, is.same.GroupB) %>% 
                     mutate(regions = n()) %>%
                     ungroup() %>%
                     pull(regions))

  # create a list of data frames, with different regions subsampling values (i.e., subsample x regions per pair-wise comparison)
  # the conditons is used to avoid subsampling more regions than there are in the biggest group (could result in an error)
  grouped_list<-list()
  regions_sampled<-c(15,20,30,40,60,80,100,200)

  #run the subsampling function for values below the maximal nuber of regions/pair:
  for (i in 1:length(regions_sampled)) { 
    ifelse(biggest_group >= regions_sampled[i], 
           grouped_list[i]<-mapply(subsample_regions, list(big_organized_dfs), regions_sampled[i], SIMPLIFY = F),
           next)
  }
  # give names to the elements in the newly filled list
  for (i in 1:length(grouped_list)) {names(grouped_list)[i]<-paste0(path_names, "_",regions_sampled[i], " regions/pairwise")}

  return(grouped_list)
}
  



   

############
#   create the samples tree
############

#distances_matrix<-lapply(grouped_list, dist_mats)
#synteny_trees<-mapply(tree_making, distances_matrix, names(distances_matrix), SIMPLIFY = FALSE)

#add the relevant ojbects to a list element. 
outlist<-list(grouped_list, grouped_list_figs, big_organized_dfs)
names(outlist)<-c("pairwise comparison tables", "pairwise comparison figures", "Final table ungrouped")
return(outlist)


test_output<-mapply(external_func, paths, pathnames, MoreArgs = list(metadata), SIMPLIFY = F)






